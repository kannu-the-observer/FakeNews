{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install newspaper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import io\n",
    "import gzip\n",
    "import csv\n",
    "import codecs\n",
    "import sys\n",
    "import newspaper\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_domain(domain):\n",
    "\n",
    "    record_list = []\n",
    "\n",
    "    print (\"[*] Trying target domain: %s\" % domain)\n",
    "    for index in index_list:\n",
    "        print (\"[*] Trying index %s\" % index)\n",
    "\n",
    "        cc_url  = \"http://index.commoncrawl.org/CC-MAIN-%s-index?\" % index\n",
    "        cc_url += \"url=%s&matchType=domain&output=json\" % domain\n",
    "\n",
    "        response = requests.get(cc_url)\n",
    "\n",
    "        if response.status_code == 200: # if the api call returns data successfully\n",
    "\n",
    "            records = response.content.splitlines()\n",
    "            count = 0\n",
    "            for record in records:\n",
    "                if count%1000 == 0:\n",
    "                    record = json.loads(record)\n",
    "                    if record[\"status\"] == \"200\": # if the record contains the link to the required archive\n",
    "                        record_list.append(record)\n",
    "                count+=1\n",
    "\n",
    "            print (\"[*] Added %d results.\" % len(records))\n",
    "\n",
    "\n",
    "    print (\"[*] Found a total of %d hits.\" % len(record_list))\n",
    "\n",
    "    return record_list\n",
    "\n",
    "#\n",
    "# Downloads a page from Common Crawl\n",
    "#\n",
    "def download_page(record):\n",
    "\n",
    "    offset, length = int(record['offset']), int(record['length'])\n",
    "    offset_end = offset + length - 1\n",
    "\n",
    "    # We'll get the file via HTTPS so we don't need to worry about S3 credentials\n",
    "    # Getting the file on S3 is equivalent however - you can request a Range\n",
    "    prefix = 'https://commoncrawl.s3.amazonaws.com/'\n",
    "\n",
    "    # We can then use the Range header to ask for just this set of bytes\n",
    "    resp = requests.get(prefix + record['filename'], headers={'Range': 'bytes={}-{}'.format(offset, offset_end)})\n",
    "\n",
    "    # The page is stored compressed (gzip) to save space\n",
    "    # We can extract it using the GZIP library\n",
    "    f = gzip.GzipFile(fileobj = io.BytesIO(resp.content))\n",
    "##\n",
    "##    # What we have now is just the WARC response, formatted:\n",
    "    data = f.read().decode(\"utf-8\")\n",
    "\n",
    "    response = \"\"\n",
    "\n",
    "    if len(data):\n",
    "        try:\n",
    "            warc, header, response = data.strip().split('\\r\\n\\r\\n', 2)\n",
    "        except:\n",
    "            pass\n",
    "##\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Trying target domain: https://theonion.com/\n",
      "[*] Trying index 2020-45\n",
      "[*] Added 12315 results.\n",
      "[*] Found a total of 10 hits.\n"
     ]
    }
   ],
   "source": [
    "domain = \"https://theonion.com/\"\n",
    "\n",
    "# list of available indices\n",
    "index_list = [\"2020-45\"] #november/december 2020\n",
    "\n",
    "record_list = search_domain(domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Retrieved 440813 bytes for https://www.theonion.com/\n",
      "[*] Retrieved 222247 bytes for https://www.theonion.com/cat-treat-package-going-on-about-delicious-creamy-cent-1840832224\n",
      "[*] Retrieved 220513 bytes for https://www.theonion.com/kfc-selling-sandwich-shaped-meteorite-1819563658\n",
      "[*] Retrieved 217959 bytes for https://www.theonion.com/paula-broadwell-crashing-on-petraeus-family-s-couch-unt-1819574200\n",
      "[*] Retrieved 220989 bytes for https://www.theonion.com/study-girls-internalize-gender-stereotypes-by-age-6-1819563455\n",
      "[*] Retrieved 534479 bytes for https://www.theonion.com/the-week-in-pictures-week-of-march-2-2020-1842015833/slides/21\n",
      "[*] Retrieved 214242 bytes for https://entertainment.theonion.com/fans-celebrate-vanna-white-s-first-show-as-guest-wheel-1840346038\n",
      "[*] Retrieved 214775 bytes for https://local.theonion.com/pumpkin-spends-summer-getting-huge-to-avoid-being-picke-1845322943\n",
      "[*] Retrieved 217025 bytes for https://politics.theonion.com/epa-chief-pruitt-welcomes-delegation-of-pollution-from-1822795444\n",
      "[*] Retrieved 223231 bytes for https://sports.theonion.com/chinese-crested-dogs-beautifully-descended-testicles-br-1819571327\n"
     ]
    }
   ],
   "source": [
    "d={}\n",
    "for record in record_list:\n",
    "    html_content = download_page(record)\n",
    "    print (\"[*] Retrieved %d bytes for %s\" % (len(html_content),record['url']))\n",
    "    article = newspaper.Article(url = ' ')\n",
    "    article.set_html(html_content)\n",
    "    article.parse()\n",
    "    if len(article.text.split()) > 100: # to filter out non-news articles\n",
    "        articleDetails = {'title': article.title, 'body': article.text}\n",
    "        d[record['url']] = articleDetails\n",
    "##    print (\"content retrieved is \")\n",
    "##    print(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('newsData.json', 'w') as fp:\n",
    "    json.dump(d, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
