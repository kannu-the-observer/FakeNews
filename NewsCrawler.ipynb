{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import io\n",
    "import gzip\n",
    "import csv\n",
    "import codecs\n",
    "import sys\n",
    "import newspaper\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_domain(domain):\n",
    "\n",
    "    record_list = []\n",
    "\n",
    "    print (\"[*] Trying target domain: %s\" % domain)\n",
    "    for index in index_list:\n",
    "        print (\"[*] Trying index %s\" % index)\n",
    "\n",
    "        cc_url  = \"http://index.commoncrawl.org/CC-MAIN-%s-index?\" % index\n",
    "        cc_url += \"url=%s&matchType=domain&output=json\" % domain\n",
    "\n",
    "        response = requests.get(cc_url)\n",
    "\n",
    "        if response.status_code == 200: # if the api call returns data successfully\n",
    "\n",
    "            records = response.content.splitlines()\n",
    "            count = 0\n",
    "            for record in records:\n",
    "                if count%1000 == 0:\n",
    "                    record = json.loads(record)\n",
    "                    if record[\"status\"] == \"200\": # if the record contains the link to the required archive\n",
    "                        record_list.append(record)\n",
    "                count+=1\n",
    "\n",
    "            print (\"[*] Added %d results.\" % len(records))\n",
    "\n",
    "\n",
    "    print (\"[*] Found a total of %d hits.\" % len(record_list))\n",
    "\n",
    "    return record_list\n",
    "\n",
    "#\n",
    "# Downloads a page from Common Crawl\n",
    "#\n",
    "def download_page(record):\n",
    "\n",
    "    offset, length = int(record['offset']), int(record['length'])\n",
    "    offset_end = offset + length - 1\n",
    "\n",
    "    # We'll get the file via HTTPS so we don't need to worry about S3 credentials\n",
    "    # Getting the file on S3 is equivalent however - you can request a Range\n",
    "    prefix = 'https://commoncrawl.s3.amazonaws.com/'\n",
    "\n",
    "    # We can then use the Range header to ask for just this set of bytes\n",
    "    resp = requests.get(prefix + record['filename'], headers={'Range': 'bytes={}-{}'.format(offset, offset_end)})\n",
    "\n",
    "    # The page is stored compressed (gzip) to save space\n",
    "    # We can extract it using the GZIP library\n",
    "    f = gzip.GzipFile(fileobj = io.BytesIO(resp.content))\n",
    "##\n",
    "##    # What we have now is just the WARC response, formatted:\n",
    "    data = f.read().decode(\"utf-8\")\n",
    "\n",
    "    response = \"\"\n",
    "\n",
    "    if len(data):\n",
    "        try:\n",
    "            warc, header, response = data.strip().split('\\r\\n\\r\\n', 2)\n",
    "        except:\n",
    "            pass\n",
    "##\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"https://theonion.com/\"\n",
    "\n",
    "# list of available indices\n",
    "index_list = [\"2020-45\"] #november/december 2020\n",
    "\n",
    "record_list = search_domain(domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={}\n",
    "for record in record_list:\n",
    "    html_content = download_page(record)\n",
    "    print (\"[*] Retrieved %d bytes for %s\" % (len(html_content),record['url']))\n",
    "    article = newspaper.Article(url = ' ')\n",
    "    article.set_html(html_content)\n",
    "    article.parse()\n",
    "    if len(article.text.split()) > 100: # to filter out non-news articles\n",
    "        articleDetails = {'title': article.title, 'body': article.text}\n",
    "        d[record['url']] = articleDetails\n",
    "##    print (\"content retrieved is \")\n",
    "##    print(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('newsData.json', 'w') as fp:\n",
    "    json.dump(d, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProjectEnv",
   "language": "python",
   "name": "projectenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
