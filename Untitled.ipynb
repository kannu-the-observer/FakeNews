{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import io\n",
    "import gzip\n",
    "import csv\n",
    "import codecs\n",
    "import sys\n",
    "import newspaper\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_domain(domain):\n",
    "\n",
    "    record_list = []\n",
    "\n",
    "    print (\"[*] Trying target domain: %s\" % domain)\n",
    "    for index in index_list:\n",
    "        print (\"[*] Trying index %s\" % index)\n",
    "\n",
    "        cc_url  = \"http://index.commoncrawl.org/CC-MAIN-%s-index?\" % index\n",
    "        cc_url += \"url=%s&matchType=domain&output=json\" % domain\n",
    "\n",
    "        response = requests.get(cc_url)\n",
    "\n",
    "        if response.status_code == 200: # if the api call returns data successfully\n",
    "\n",
    "            records = response.content.splitlines()\n",
    "            count = 0\n",
    "            for record in records:\n",
    "                if count%100 == 0:\n",
    "                    record = json.loads(record)\n",
    "                    if record[\"status\"] == \"200\": # if the record contains the link to the required archive\n",
    "                        record_list.append(record)\n",
    "                count+=1\n",
    "\n",
    "            print (\"[*] Added %d results.\" % len(records))\n",
    "\n",
    "\n",
    "    print (\"[*] Found a total of %d hits.\" % len(record_list))\n",
    "\n",
    "    return record_list\n",
    "\n",
    "#\n",
    "# Downloads a page from Common Crawl\n",
    "#\n",
    "def download_page(record):\n",
    "\n",
    "    offset, length = int(record['offset']), int(record['length'])\n",
    "    offset_end = offset + length - 1\n",
    "\n",
    "    # We'll get the file via HTTPS so we don't need to worry about S3 credentials\n",
    "    # Getting the file on S3 is equivalent however - you can request a Range\n",
    "    prefix = 'https://commoncrawl.s3.amazonaws.com/'\n",
    "\n",
    "    # We can then use the Range header to ask for just this set of bytes\n",
    "    resp = requests.get(prefix + record['filename'], headers={'Range': 'bytes={}-{}'.format(offset, offset_end)})\n",
    "\n",
    "    # The page is stored compressed (gzip) to save space\n",
    "    # We can extract it using the GZIP library\n",
    "    f = gzip.GzipFile(fileobj = io.BytesIO(resp.content))\n",
    "##\n",
    "##    # What we have now is just the WARC response, formatted:\n",
    "    data = f.read().decode(\"utf-8\")\n",
    "\n",
    "    response = \"\"\n",
    "\n",
    "    if len(data):\n",
    "        try:\n",
    "            warc, header, response = data.strip().split('\\r\\n\\r\\n', 2)\n",
    "        except:\n",
    "            pass\n",
    "##\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a domain namehttps://www.hindustantimes.com\n",
      "Enter the index in YYYY-WW format2020-45\n",
      "[*] Trying target domain: https://www.hindustantimes.com\n",
      "[*] Trying index 2020-45\n",
      "[*] Added 14275 results.\n",
      "[*] Found a total of 132 hits.\n"
     ]
    }
   ],
   "source": [
    "domain = input(\"Enter a domain name\")\n",
    "# domain = \"https://www.hindustantimes.com/\"\n",
    "\n",
    "# list of available indices\n",
    "# index_list = [\"2020-45\"] #november/december 2020\n",
    "index_list = []\n",
    "index = input(\"Enter the index in YYYY-WW format\")\n",
    "index_list.append(index)\n",
    "\n",
    "record_list = search_domain(domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "d={}\n",
    "for record in record_list:\n",
    "    html_content = download_page(record)\n",
    "#     print (\"[*] Retrieved %d bytes for %s\" % (len(html_content),record['url']))\n",
    "    article = newspaper.Article(url = ' ')\n",
    "    article.set_html(html_content)\n",
    "    article.parse()\n",
    "    if len(article.text.split()) > 100: # to filter out non-news articles\n",
    "        articleDetails = {'title': article.title, 'body': article.text, 'label': \"True\"}\n",
    "        d[record['url']] = articleDetails\n",
    "##    print (\"content retrieved is \")\n",
    "##    print(html_content)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('newsData.json', 'r') as f:\n",
    "        newsData = json.load(f) \n",
    "except:\n",
    "    with open('newsData.json', 'w') as f:\n",
    "        newsData = {}\n",
    "for url in d:\n",
    "    newsData[url] = d[url]\n",
    "with open('newsData.json', 'w') as f:\n",
    "    json.dump(newsData, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len 715\n",
      "len 909\n",
      "len 222\n",
      "len 217\n",
      "len 232\n",
      "len 660\n",
      "len 463\n",
      "len 502\n",
      "len 434\n",
      "len 387\n",
      "len 438\n",
      "len 406\n",
      "len 290\n",
      "len 439\n",
      "len 364\n",
      "len 317\n",
      "len 351\n",
      "len 528\n",
      "len 371\n",
      "len 260\n",
      "len 124\n",
      "len 470\n",
      "len 516\n",
      "len 414\n",
      "len 232\n",
      "len 841\n",
      "len 191\n",
      "len 361\n",
      "len 771\n",
      "len 539\n",
      "len 239\n",
      "len 304\n",
      "len 227\n",
      "len 464\n",
      "len 180\n",
      "len 307\n",
      "len 246\n",
      "len 316\n",
      "len 238\n",
      "len 313\n",
      "len 572\n",
      "len 267\n",
      "len 409\n",
      "len 293\n",
      "len 155\n",
      "len 475\n",
      "len 155\n",
      "len 301\n",
      "len 534\n",
      "len 330\n",
      "len 360\n",
      "len 220\n",
      "len 524\n",
      "len 426\n",
      "len 256\n",
      "len 1104\n",
      "len 454\n",
      "len 397\n",
      "len 311\n",
      "len 621\n",
      "len 321\n",
      "len 230\n",
      "len 682\n",
      "len 382\n",
      "len 205\n",
      "len 382\n",
      "len 435\n",
      "len 467\n",
      "len 272\n",
      "len 282\n",
      "len 415\n",
      "len 478\n",
      "len 289\n",
      "len 554\n",
      "len 404\n",
      "len 384\n",
      "len 197\n",
      "len 273\n",
      "len 270\n",
      "len 375\n",
      "len 279\n",
      "len 320\n",
      "len 258\n",
      "len 223\n",
      "len 236\n",
      "len 212\n",
      "len 180\n",
      "len 153\n",
      "len 441\n",
      "len 307\n",
      "len 338\n",
      "len 372\n",
      "len 279\n",
      "len 276\n",
      "len 332\n",
      "len 540\n",
      "len 150\n",
      "len 134\n",
      "len 295\n",
      "len 976\n",
      "len 110\n",
      "len 663\n",
      "len 427\n",
      "len 479\n",
      "len 358\n",
      "len 324\n",
      "len 407\n",
      "len 231\n",
      "len 407\n",
      "len 212\n",
      "len 469\n",
      "len 293\n",
      "len 473\n",
      "len 758\n",
      "len 440\n",
      "len 455\n",
      "len 438\n",
      "len 242\n",
      "len 176\n",
      "len 383\n",
      "len 443\n",
      "len 436\n",
      "len 984\n",
      "len 444\n",
      "len 1034\n",
      "len 283\n",
      "len 571\n",
      "len 357\n",
      "len 353\n"
     ]
    }
   ],
   "source": [
    "newsData = {}\n",
    "with open('newsData.json', 'r') as f:\n",
    "    newsData = json.load(f)\n",
    "for url in newsData:\n",
    "    print('len', len(newsData[url]['body'].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProjectEnv",
   "language": "python",
   "name": "projectenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
